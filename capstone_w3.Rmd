---
title: "Capstone Repo - week 3 modeling"
author: "J Balvers"
date: "28 mei 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd('D:/Rwd/capstone')

```

# Task 4 - prediction model

The goal of this exercise is to build and evaluate your first predictive model. You will use the n-gram and backoff models you built in previous tasks to build and evaluate your predictive model. The goal is to make the model efficient and accurate.

Tasks to accomplish

1. Build a predictive model based on the previous data modeling steps - you may combine the models in any way you think is appropriate.
2. Evaluate the model for efficiency and accuracy - use timing software to evaluate the computational complexity of your model. Evaluate the model accuracy using different metrics like perplexity, accuracy at the first word, second word, and third word.
Questions to consider 

- How does the model perform for different choices of the parameters and size of the model?
- How much does the model slow down for the performance you gain?
- Does perplexity correlate with the other measures of accuracy?
- Can you reduce the size of the model (number of parameters) without reducing performance?

# Building the model

The n-gram models created in the previous steps are 2-column data frames. To create a prediction model, it seems appropriate to separate the words from 'Text' column into separate columns and then train a model for the last column.


```{r model, warning=FALSE}
# first try: the caret package
library(caret)
library(RWeka)
library(tidyr)

# split the n-grams in columns excluding the frequency
bigram_split <- separate(bigram_clean, Text, into = c("w1","outcome"))
trigram_split <- separate(trigram_clean, Text, into = c("w1","w2","outcome"))
fourgram_split <- separate(fourgram_clean, Text, into = c("w1","w2","w3","outcome"))

save(bigram_split, trigram_split, fourgram_split, file= "n_grams_split.RData")

```


###########################################
### WORK IN PROGRESS ######################
###########################################

## Model training process

Next we train a small (n=100) model, because this training method results in vector size errors if entire n-grams are used.

# Some research is needed to select another model
other potential methods:
- ... 

# And to compress the ngrams in size before training the model
- use cutoff: e.g. remove n-grams with count of < 10/20/50/100?
- or leave in only the top x n-grams (100 will be a bit too small I imagine)
- or: leave out all words that are very uncommon, eg by filtering on the top x words from the unigram model!!

The simplest way is to include only n-grams that occur a certain number of times, by using a cutoff for the minimum frequency.

``` {r n-gram_cutoff}
# Find how many n-grams occur at least 100 times:
x <- 100; i <- 1; while(x >= 100) {x <- bigram_split$Freq[i]; i <- i + 1}; print(i)

# This results in:
  # bigrams: 1.092
  # trigrams: 108
  # fourgrams: 5



```

Another way to look at the n-grams is by relative frequency: the n-grams that occur most often are more useful to include in the model than n-grams that are not so common. Using the Frequency column we can calculate the sum of frequencies for the n-th n-gram that covers for example 80 or 90% of all n-grams.

``` {r n-gram_frequency}
# Find the row (is number of n-grams) that cover for example 80 or 90% of all n-grams in the data (looking at frequency)

# bigram
x <- 0.1; i<- 1; while(x < 0.8) { x <- (sum(bigram_split$Freq[1:i]) / sum(bigram_split$Freq)); i <- i + 1}; print(i)

# This returns 
  # row 47.681 for an 80% coverage -> this n-gram has a frequency of 10
  # row 67.436 for a 90% coverage

# trigram
x <- 0.1; i<- 1; while(x < 0.8) { x <- (sum(trigram_split$Freq[1:i]) / sum(trigram_split$Freq)); i <- i + 1}; print(i)

# This returns 
  # row 84.714 for an 80% coverage-> this n-gram has a frequency of 6
  # row 127.761 for a 90% coverage

# fourgram
x <- 0.1; i<- 1; while(x < 0.8) { x <- (sum(fourgram_split$Freq[1:i]) / sum(fourgram_split$Freq)); i <- i + 1}; print(i)

# This returns 
  # row 73.007 for an 70% coverage -> this n-gram has a frequency of 8
  # row 99.024 for an 80% coverage
  # row 194.659 for a 90% coverage
```
This is still too many lines to efficiently train the model. And at these percentages we will include n-grams that only have 10 occurences.

A possible way to filter n-grams is by only including those n-grams that have words (or of which all words) are the most common in the unigram data. That is: the frequency list of single words.

``` {r unigram_filtering}
n <- 10000; sum_n <- sum(unigram_clean$Freq[1:n]); sum <- sum(unigram_clean$Freq); f <- sum_n/sum; f

# This results in: 10.000 most frequent words cover 95.2 % of the unigram dataset. (top 10 cover 20.5% of the dataset)

```

# N-gram data selection

From the above we (quite arbitrarily) choose to use .....
====> the selection can be made by trying out what number of 4-grams the training process still supports.


# back-off strategy and unseen n-grams
Back-off strategy would probably be:
1. start with 4-gram. If not found (or if likelihood is < x) go to lower level n-gram
2. if no suggestion from bigram, then we have an unseen n-gram. For now we can have the model return "UNSEEN" or something similar to see how often this happens.
  - From literature: 'smoothing' can be a solution (literature points to best results with Kneser-Ney smoothing)

# Tested on 04-06:
model_trigram <- train(outcome ~ ., data=trigram_split[1:1000,1:3], method='gbm')


```` {r model_training}

# Find out which model scale (number of n-grams) stil works and then test outcome of predictions for top 5 n-grams

# use this to time the function:
system.time()

# train a small-scale prediction model (100 rows)
model_bigram <- train(outcome ~ ., data=bigram_split[1:100,1:2], method='J48')
model_trigram <- train(outcome ~ ., data=trigram_split[1:100,1:3], method='J48')

# this fourgram model is based on only the first 10 lines because of the above error
model_fourgram <- train(outcome ~ ., data=fourgram_split[1:10,1:4], method='J48')

# perform some predictions on test data
test_bigram <- data.frame(
    w1 = c("I", "You" ,"think" ,"I" )
)
test_trigram <- data.frame(
    w1 = c("I", "I" ,"I" ,"I" ), 
    w2 = c("think", "dont", "know", "feel" )
)
test_fourgram <- data.frame(
    w1 = c("little", "I" ,"gaston" ,"I" ), 
    w2 = c("boy", "must", "south", "feel" ),
    w3 = c("big", "say", "carolina", "like" )
)


pred_bigram <- predict(model_bigram, test_bigram)
pred_trigram <- predict(model_trigram, test_trigram)
pred_fourgram <- predict(model_fourgram, test_fourgram)

```

# Next thoughts


- select a much smaller subset of n-grams for model building
	- and time model speed and calculate model accuracy
	- then evaluate if larger set of n-grams is required
- create a dictionary that covers eg 90% of text and exclude all other words
- 

