---
title: "Capstone Repo - week 3 modeling"
author: "J Balvers"
date: "28 mei 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd('D:/Rwd/capstone')

```

# Task 4 - prediction model

The goal of this exercise is to build and evaluate your first predictive model. You will use the n-gram and backoff models you built in previous tasks to build and evaluate your predictive model. The goal is to make the model efficient and accurate.

Tasks to accomplish

1. Build a predictive model based on the previous data modeling steps - you may combine the models in any way you think is appropriate.
2. Evaluate the model for efficiency and accuracy - use timing software to evaluate the computational complexity of your model. Evaluate the model accuracy using different metrics like perplexity, accuracy at the first word, second word, and third word.
Questions to consider 

- How does the model perform for different choices of the parameters and size of the model?
- How much does the model slow down for the performance you gain?
- Does perplexity correlate with the other measures of accuracy?
- Can you reduce the size of the model (number of parameters) without reducing performance?

# Building the model

The n-gram models created in the previous steps are 2-column data frames. To create a prediction model, it seems appropriate to separate the words from 'Text' column into separate columns and then train a model for the last column.


```{r model, warning=FALSE}
# first try: the caret package
library(caret)
library(RWeka)
library(tidyr)

# split the n-grams in columns excluding the frequency
bigram_split <- separate(bigram_clean, Text, into = c("w1","outcome"))
trigram_split <- separate(trigram_clean, Text, into = c("w1","w2","outcome"))
fourgram_split <- separate(fourgram_clean, Text, into = c("w1","w2","w3","outcome"))

```

# WORK IN PROGRESS

## Model training process

Next we train a small (n=100) model, because this training method results in vector size errors if entire n-grams are used.

# Some research is needed to select another model
other potential methods:
- ... 

# And to compress the ngrams in size before training the model
- use cutoff: e.g. remove n-grams with count of < 10/20/50/100?
- or leave in only the top x n-grams (100 will be a bit too small I imagine)
- or: leave out all words that are very uncommon, eg by filtering on the top x words from the unigram model!!

### n <- 10000; sum_n <- sum(unigram_clean$Freq[1:n]); sum <- sum(unigram_clean$Freq); f <- sum_n/sum; f
This results in: 10.000 most frequent words cover 95.2 % of the unigram dataset. (top 10 cover 20.5% of the dataset)

# back-off strategy and unseen n-grams
Back-off strategy would probably be:
1. start with 4-gram. If not found (or if likelihood is < x) go to lower level n-gram
2. if no suggestion from bigram, then we have an unseen n-gram. For now we can have the model return "UNSEEN" or something similar to see how often this happens.
  - From literature: 'smoothing' can be a solution (literature points to best results with Kneser-Ney smoothing)


```` {r model_training}
# train a small-scale prediction model (100 rows)
model_bigram <- train(outcome ~ ., data=bigram_split[1:100,1:2], method='J48')
model_trigram <- train(outcome ~ ., data=trigram_split[1:100,1:3], method='J48')

# something goes wrong here: 
    #Warning message:
    #In doTryCatch(return(expr), name, parentenv, handler) :
    #  restarting interrupted promise evaluation
    #Error in system.time - time : non-numeric argument to binary operator
   
        #  time <- system.time()
        #  model_fourgram <- train(outcome ~ ., data=fourgram_split[1:100,1:4], method='J48')
        #  system.time() - time

    # this fourgram model is based on only the first 10 lines because of the above error
    model_fourgram <- train(outcome ~ ., data=fourgram_split[1:10,1:4], method='J48')

# perform some predictions on test data
test_bigram <- data.frame(
    w1 = c("I", "You" ,"think" ,"I" )
)
test_trigram <- data.frame(
    w1 = c("I", "I" ,"I" ,"I" ), 
    w2 = c("think", "dont", "know", "feel" )
)
test_fourgram <- data.frame(
    w1 = c("little", "I" ,"gaston" ,"I" ), 
    w2 = c("boy", "must", "south", "feel" ),
    w3 = c("big", "say", "carolina", "like" )
)


pred_bigram <- predict(model_bigram, test_bigram)
pred_trigram <- predict(model_trigram, test_trigram)
pred_fourgram <- predict(model_fourgram, test_fourgram)

```

# Next thoughts


- select a much smaller subset of n-grams for model building
	- and time model speed and calculate model accuracy
	- then evaluate if larger set of n-grams is required
- create a dictionary that covers eg 90% of text and exclude all other words
- 

Suggestion for model workings: (from: https://rstudio-pubs-static.s3.amazonaws.com/139244_a5629dbf418f465ab825c063f22535d5.html)

1, 2, 3 and 4 n-gram tables are stored as text files.
Only n-grams that have fequency higher or equal to 2 are kept in the model.
The n-gram tables are loaded from the text files.
For a string of text that is input into the predictor the prediction algorithm performs a search on each n-gram table, starting with the 4-gram table.
From the imput text, the last three terms are obtained and searched in the 4-gram table. If one or more matches are found, then the algorithm outputs the best predictions for the next word given those three terms.
If no match is found in the 4-gram table, then the search continues in the 3-gram table using the last two words from the input. And so on. If no match is found, the prediction is then the most common one-gram (single terms).

``` {r quiz2}

inputData <- c(
    "The guy in front of me just bought a pound of bacon, a bouquet, and a case of",
    "You're the reason why I smile everyday. Can you follow me please? It would mean the",
    "Hey sunshine, can you follow me and make me the",
    "Very early observations on the Bills game: Offense still struggling but the",
    "Go on a romantic date at the",
    "Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my",
    "Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some",
    "After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little",
    "Be grateful for the good times and keep the faith during the",
    "If this isn't the cutest thing you've ever seen, then you must be")
```

Answers:
1. beer
2. world
3. happiest
4. players = FALSE, crowd = FALSE, defense
5. movies = FALSE, mall = FALSE, beach
6. phone = FALSE, way
7. time
8. ears = FALSE, toes = FALSE, fingers
9. bad
10. insane